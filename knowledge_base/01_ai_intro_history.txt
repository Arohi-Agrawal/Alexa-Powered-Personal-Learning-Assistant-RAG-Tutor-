# TOPIC: 01 Introduction and History of AI

***

# Section 1: Foundational Definitions

## 1.1 Artificial Intelligence (AI)
**Artificial Intelligence (AI)** is the broad field of computer science dedicated to building systems capable of performing tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. The core goal of AI is to design machines that can simulate intelligent human behavior.

## 1.2 Machine Learning (ML)
**Machine Learning (ML)** is a key subset of AI. It involves developing algorithms that allow a machine to learn from **data** without being explicitly programmed. The machine builds a model based on **sample data** (training data) to make predictions or classify information. The model improves its performance as it is exposed to more data.

## 1.3 Deep Learning (DL)
**Deep Learning (DL)** is a specialized subset of ML. It uses **Artificial Neural Networks** composed of multiple layers (**deep neural networks**) to analyze complex data like images, sound, and text. DL is responsible for the rapid advancements in fields like computer vision and natural language processing (NLP).

***

# Section 2: AI Classification and Goals

## 2.1 AI Classification by Capability
- **Narrow AI (Weak AI):** AI designed and trained to perform a single, specific task. This includes virtual assistants (like Alexa), facial recognition software, and recommendation engines. **All current, real-world AI systems fall into this category.**
- **General AI (Strong AI / Human-Level AI):** Hypothetical AI possessing the intellectual ability to understand, learn, and apply its intelligence to solve *any* problem, just like a human being.
- **Superintelligence:** Hypothetical AI that would surpass human intellectual capability in virtually every field, including scientific creativity and problem-solving.

## 2.2 The Turing Test
The **Turing Test**, proposed by Alan Turing in 1950, is a measure of a machine's ability to exhibit intelligent behavior equivalent to a human. The test involves a human judge engaging in a natural language conversation (via text) with both a human and a machine. If the judge cannot reliably distinguish the machine from the human, the machine is said to have passed the test.

***

# Section 3: Key Historical Milestones

## 3.1 The Dartmouth Workshop (1956)
This event is widely considered the **birth of AI** as an academic field. John McCarthy, one of the founders of the discipline, coined the term **"Artificial Intelligence"** at this conference, bringing together researchers to discuss the creation of thinking machines.

## 3.2 The Perceptron (1957)
Invented by Frank Rosenblatt, the **Perceptron** was one of the first computational models of a biological neuron and an early form of a simple artificial neural network. It was designed for binary classification (separating data into two categories).

## 3.3 Early Breakthroughs and the AI Winters
Following early excitement (1950s–1960s), expectations outpaced reality, leading to periods known as the **AI Winters** (1974–1980s and late 1980s). Funding and research interest dropped sharply due to limitations in computing power and the inability of early AI models to solve complex, real-world problems.

## 3.4 The Deep Learning Revolution (2000s - Present)
The current resurgence of AI is driven by **three factors**: massive increases in available **data**, highly powerful **GPUs** (Graphics Processing Units) for parallel computation, and advances in **Deep Learning algorithms**. Breakthroughs like **DeepMind's AlphaGo** (2016), which defeated a world champion at the complex game of Go, demonstrated AI's ability to master human-level strategies.